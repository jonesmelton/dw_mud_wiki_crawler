# DwWikiScraper
Web crawler to scrape the [discworld mud wiki](https://dwwiki.mooo.com).
Implemented with [crawly](https://github.com/elixir-crawly/crawly).

## setup
Assumes you have elixir & erlang runtime installed.
1. `pull repo`
2. `mix deps.get`

## run the crawler
`iex -S mix run -e "Crawly.Engine.start_spider(WikiScraper)"`

In-progress crawl will be displayed in console. Crawly also supplies a basic UI at `http://localhost:4001`.

Crawly keeps a record of crawls in an [erlang dets](https://www.erlang.org/doc/man/dets.html) file at `dets_simple_storage`. I don't know how much it keeps in there or whether it uses it to resume canceled crawls.

## config
Crawly config file is `config/config.exs`.

These settings are default generated by crawly, except the addition of the custom ecto pipeline.

Modify user agent to indicate some way of contacting you. I used my mud character name.
Concurrent requests is the set to the default of 8, I haven't tried increasing it.
I don't want to hit a rate limiter or crush the wiki.

The sqlite db name is hard coded in `DwWikiScraper.SQlitePipeline.Repo` if you want to change it.

## info
The wiki has become a critical resource for discworld mud players but I've never seen anyone
admit to knowing who runs it. It's down fairly regularly, and I worry about it disappearing
permanently, as has happened with previous player-run resources.

This repo does not include the db file resulting from a crawl. If for some reason you need one
contact me.

As of june 2023 a crawl was 
